---
layout: post
title: Artificial neurons as ridge functions
description: Approximation theory of ridge functions
intro: Artificial neurons are a special case of ridge functions, so linear combinations of ridge functions are a generalization of multilayer perceptrons with one hidden layer. The approximation theory of ridge functions is an important gateway to the study of approximation capabilities of neural networks.
---
<!-- improve intro -->
<section>
  <h1 id="mlp">Multilayer perceptron</h1>
  <p>
    The multilayer perceptron (MLP) is a fundamental neural network model consisting of a sequence of <em>layers</em>.
    Every layer is composed by <em>neurons</em>, the fundamental processing unit of the network.
    Every artificial neuron in the intermediate <em>hidden layers</em> processes the outputs of the previous layer $\bold{x}$ with a non-linear function $N: \mathbb{R}^{n} \longrightarrow \mathbb{R}$ such that
    $$
    N(\bold{x}) = \sigma\left(\sum_{j=1}^{n} w_j x_j - \theta \right) =
    \sigma(\bold{w} \cdot \bold{x} - \theta),
    $$
    where $\bold{w}$ and $\theta$ are parameters that change for every neuron.
    The non-linearity of $N$ comes from the <em>activation function</em> $\sigma$, a non-linear function
    that is the same for every neuron in the network.
    <small>
      In the neural network literature, the values $w_{i}$ are called <em>weights</em> and they model the strength of the synapse linking the $i$-th neuron of the previous layer with the current neuron. $\theta$ is called <em>bias</em> and it recalls the threshold potential involved in the firing of biologic neurons.
    </small>
  </p>
  <p>
    In the final layer, said the <em>output layer</em>, neurons operates differently. They just perform a linear combination of their inputs. Therefore, the output $y$ of multilayer perceptron with a single hidden layer with $r$ units is
    $$
    y = \sum_{i=1}^{r} c_i N_i(\bold{x}) = \sum_{i=1}^{r} c_i \sigma\left(\bold{w}^{i} \cdot \bold{x} - \theta_i\right).
    $$
    It is possible to stack more hidden layers of various sizes to obtain a deeper network.
    We will focus on this model because it can already approximate a very large class of functions.
  </p>
</section>
<section>
  <h1 id="ridge">Ridge functions</h1>
  <p>
    The function $N(\bold{x})$ has a particular form: it is the composition of a univariate function $\sigma$ with the inner product, one of the simplest multivariate functions. Functions of such form are called ridge functions.
  </p>
  <p style="background-color:#d8deea; padding:10px;">
    <strong style="color:#434498; padding-bottom:10px">DEFINITION</strong></br>
    A <strong>ridge function</strong> is a function $F: \mathbb{R}^{n} \longrightarrow \mathbb{R}$
    of the form
    $$
    F(\bold{x}) = f(a_1 x_1 + \mathellipsis + a_n x_n) = f(\bold{a} \cdot \bold{x}),
    $$
    where $f: \mathbb{R} \longrightarrow \mathbb{R}$ is a univariate function and
    $\bold{a} = (a_1, \mathellipsis, a_n) \in \mathbb{R}^{n} - \{\bold{0}\}$ is a fixed vector.
  </p>
  <p>
    The vector $\bold{a}$ is called the <em>direction</em> because a ridge function is a multivariate function constant on the parallel hyperplanes orthogonal to $\bold{a}$. In fact, these hyperplanes are defined by the equation $\bold{a} \cdot \bold{x} = c$, with $c \in \mathbb{R}$.
  </p>
  <p>
    For a given direction $\bold{a}$, we denote the set of ridge functions with direction $\bold{a}$ $$\mathcal{M}(\bold{a}) = \{f(\bold{a} \cdot \bold{x}) f: \mathbb{R} \longrightarrow \mathbb{R}\}.$$
    Since the function $f$ can be arbitrarily scaled, it follows that if $\bold{a} = \lambda \bold{b}$ for some $\lambda \in \mathbb{R} - \{0\}$, then $\mathcal{M}(\bold{a}) = \mathcal{M}(\bold{b})$.
  </p>
  <p>
    For a set $\Omega \subseteq \mathbb{R}^{n}$, we define
    $$
    \mathcal{M}(\Omega) = \text{span}\{f(\bold{a} \cdot \bold{x}) f: \mathbb{R} \longrightarrow \mathbb{R}, \bold{a} \in \Omega\}.
    $$
    A <em>span</em> of a set $S$ is the set of linear combinations of elements of $S$, then
    every $F \in \mathcal{M}(\Omega)$ has the form
    $$
    F(\bold{x}) = \sum_{i=1}^{r} c_i f_i(\bold{a}^{i} \cdot \bold{x}).
    $$
    We can notice that $\mathcal{M}(\mathbb{R}^{n})$ includes the set of MLPs with one hidden layer, but MLPs have the additional constraint $f_i = \sigma$ for every $i = 1, \mathellipsis, r$.
    We are interested in the theory of approximation of sets $\mathcal{M}(\Omega)$ to better characterize the class of functions defined by MLPs.
  </p>
</section>
<section>
  <h1 id="discrete-convolution">Discrete convolution</h1>
  <p>
    The convolution operation is a well-known building block of deep neural networks. We are going to prove some properties of the discrete convolution. We focus our attention on the monodimensional convolution, but all theorems generalize to multiple dimensions.
  </p>
  <p style="background-color:#d8deea; padding:10px;">
    <strong style="color:#434498; padding-bottom:10px">DEFINITION</strong></br>
    Let $f$, $g$ be two real signals, the <strong>discrete convolution</strong> of $f$ and $g$ is
    $$
    (f * g)[n] = \sum_{k=-\infty}^{+\infty}f[k]g[n-k] \quad \forall\: n \in \mathbb{Z}.
    $$
  </p>
  <p>
    The meaning of such a formula is not immediately understandable. It is easier to understand it through $\overline{g}[n] = g[-n]$, the flipped version of $g$.
    When $g$ has finite support in $\{0, ..., m-1\}$, we can visualize the convolution as the scalar product of $\overline{g}$ with every window of $m$ elements of the signal $f$.
    <small>
      The support of a function $f: D \longrightarrow \mathbb{R}$ is the subset of the domain $D$ containing those elements which are not mapped to zero: </br>
      $\text{supp}(f) = \{x \in D | f(x) \neq 0\}$.
    </small>
  </p>
  <p>
    In general, the summation in the convolution definition is not always finite.
    Recalling Holder's inequality, we can find a sufficient condition to establish whether the convolution is well-defined for every element.
  </p>
  <p style="background-color:#d8deea; padding:10px;">
    <strong style="color:#434498; padding-bottom:10px">THEOREM</strong></br>
    Let $f \in \ell^{p}(\mathbb{R})$ and $g \in \ell^{q}(\mathbb{R})$ be signals
    with $p, q \in [1, \infty]$ such that
    $$
    \frac{1}{p} + \frac{1}{q} = 1 \quad \text{or} \quad \{p, q\} = \{1, \infty\},
    $$
    then $f * g$ is bounded.
    <small>
      For every $p > 1$, $\ell^{p}(\mathbb{R})$ is the space of $p$-summable sequences,
      i.e. $f \in \ell^{p}(\mathbb{R})$ if and only if
      $\left(\sum_{k=-\infty}^{\infty} f[k]^p\right)^{\frac{1}{p}} = ||f||_{p}< +\infty$.</br>
      $\ell^{\infty}(\mathbb{R})$ is the space of bounded sequences.
    </small>
  </p>
  <h3>Proof</h3>
  <p>
    $f * g$ is a bounded signal if $\exist \:M \in \mathbb{R}$ such that
    $$
    |(f*g)[n]| \leq M \quad \forall\:n \in \mathbb{Z}.
    $$
    Using Hölder's inequality and with a simple change of variable, we prove the theorem:
    <small>
    Hölder's inequality states that $||fg||_{1} \leq ||f||_{p}||g||_{q}$
    if $p$ and $q$ satisfy the condition of the theorem.
    The special case $p=q=2$ gives the Cauchy–Schwarz inequality.
    </small>
    $$
    \begin{aligned}
        |(f*g)[n]| & \leq \sum_{k=-\infty}^{+\infty}|f[k]g[n-k]| \\
        & \leq \left[\sum_{k=-\infty}^{+\infty}|f[k]|^{p}\right]^{\frac{1}{p}}
        \left[\sum_{k=-\infty}^{+\infty}|g[n-k]|^{q}\right]^{\frac{1}{q}} & \textit{\footnotesize Hölder's ineq.} \\
        & = \left[\sum_{k=-\infty}^{+\infty}|f[k]|^{p}\right]^{\frac{1}{p}}
        \left[\sum_{k'=-\infty}^{+\infty}|g[k']|^{q}\right]^{\frac{1}{q}} & {\footnotesize k' = n - k } \\
        & = ||f||_{p}||g||_{q}. & \square
    \end{aligned}
    $$
  </p>
  <p>
    Two remarkable properties of the convolution are commutativity and linearity.
    These two properties are sufficient to explain why convolutions are a fundamental component
    of many processing systems.
  </p>
  <p style="background-color:#d8deea; padding:10px;">
    <strong style="color:#434498; padding-bottom:10px">THEOREM</strong></br>
    Let $f, g$ be two real signals such that its convolution is well-defined, then
    $$
    (f * g)[n] = (g * f)[n] \quad \forall\: n \in \mathbb{Z}.
    $$
    That means that the convolution is a <strong>commutative</strong> operation.
  </p>
  <h3>Proof</h3>
  <p>
    For any fixed $n \in \mathbb{Z}$, let $j = n-k$, then $k = n-j$.
    $$
    \begin{aligned}
    (f * g)[n] & = \sum_{k=-\infty}^{+\infty}f[k]g[n-k] \\
    & = \sum_{j=-\infty}^{+\infty}g[j]f[n-j] = (g * f)[n]. & \hspace*{6em} \square
    \end{aligned}
    $$
  </p>
  <p style="background-color:#d8deea; padding:10px;">
    <strong style="color:#434498; padding-bottom:10px">THEOREM</strong></br>
    Let $f, g, h$ be real signals and $a, b \in \mathbb{R}$. If the convolutions below
    are all well-defined, then we have
    $$
    ((af + bg) * h)[n] = a(f * h)[n] + b(g * h)[n] \quad \forall\: n \in \mathbb{Z}.
    $$
    Thus the convolution is a <strong>linear</strong> operation.
  </p>
</section>
<section>
    <h1 id="operators">Operators</h1>
    <p>
        To extract information from a stream of data, we have to process it.
        Every operation applied to a signal is associated with an operator.
        For example, we can use operators to reduce noise, to recognize features,
        to detect peaks or discontinuities.
    </p>
    <p style="background-color:#d8deea; padding:10px;">
      <strong style="color:#434498; padding-bottom:10px">DEFINITION</strong></br>
      A discrete operator $L$ is <strong>linear</strong> if $\forall\: a, b \in \mathbb{R}$
      $$L(a \cdot f + b \cdot g)[n] = a \cdot L(f)[n] + b \cdot L(g)[n].$$
    </p>
    <p style="background-color:#d8deea; padding:10px;">
      <strong style="color:#434498; padding-bottom:10px">DEFINITION</strong></br>
      A discrete operator is <strong>shift-invariant</strong> if $\forall\: p \in \mathbb{Z}$
      $$L(f[n-p]) = L(f)[n-p].$$
    </p>
    <p>
      When shift-invariance holds, a shift of the signal causes a corresponding displacement of the result of the operator. For example, element-wise operations are trivially shift-invariant.
      The consequence is that the result of a shift-invariant operator does not depend on the absolute position of elements in a signal, only the relative position of values matters.
    </p>
    <p>
      This new perspective shades some light on why this property is of paramount importance. Most processing operations should not depend on the absolute position of the values in the data because, in most situations, this position is arbitrary. For example, when we analyze an image, we do not want to rely on the absolute location of pixels because those positions change as soon as the image is cut or resized. The same applies to sound waves or time series.
    </p>
</section>
<section>
    <h1 id="operators-convolution">Operators and convolution</h1>
    <p>
      There is a tight relationship between linear shift-invariant operators and convolution.
      To reveal this connection, we need to recall the discrete Dirac.
      Translating the discrete Dirac by $p$, we obtain
      $$
      \delta[n - p] =
      \begin{cases}
      1 & n = p \\
      0 & n \neq p
      \end{cases}.
      $$
      Using this expression, we can represent every signal as a linear combination of Dirac signals
      $$
      f[n] = \sum_{p = -\infty}^{+\infty} f[p] \cdot \delta[n - p].
      $$
      Now we have all the elements necessary to prove the main theorem of the article.
    </p>
    <p style="background-color:#d8deea; padding:10px;">
      <strong style="color:#434498; padding-bottom:10px">THEOREM</strong></br>
      A discrete operator $L$ is linear and shift-invariant if and only if it exists a discrete
      signal $h[n]$ such that for every signal $f[n]$
      $$
      L(f)[n] = (f * h)[n].
      $$
    </p>
    <h3>Proof</h3>
    <p>
        If $L$ is linear and shift-invariant, we have to prove the existence of $h[n]$.
        Setting $h[n] = L(\delta)[n]$, we have
        $$
        \begin{aligned}
        L(f)[n] & = L\left(\sum_{p = -\infty}^{+\infty} f[p] \cdot \delta[n - p]\right) & \\
        & = \sum_{p = -\infty}^{+\infty} f[p] \cdot L(\delta[n - p]) & \textit{\footnotesize linearity} \\
        & = \sum_{p = -\infty}^{+\infty} f[p] \cdot L(\delta)[n - p] & \textit{\footnotesize shift-invariance}\\
        & = \sum_{p = -\infty}^{+\infty} f[p] \cdot h[n - p] = (f * h)[n]. &
        \end{aligned}
        $$
        To prove the inverse implication, we have to show that for every signal $h[n]$,
        $L(f) = f * h$ is a linear shift-invariant operator.
        The linearity comes from the linearity of the convolution.
        Now, we show that the resulting operation is also shift-invariant.
        $$
        \begin{aligned}
            L(f[n-p]) & = \sum_{k = -\infty}^{+\infty} f[k - p] \cdot h[n - k]) & \\
            & = \sum_{k' = -\infty}^{+\infty} f[k'] \cdot h[n - p - k']) & \qquad {\footnotesize k' = k - p} \\
            & = L(f)[n - p]. & \square
        \end{aligned}
        $$
    </p>
    <p>
      This theorem shows that convolutions are used in many applications because there is no alternative to express linear shift-invariant operations.
      We have already examined why shift-invariance is frequently an essential property. While theoretical considerations implied the importance of shift-invariance, linear operators are prevalent for practical reasons.
      Most of the fundamental operators, like the ones associated with integration and differentiation, are linear. Furthermore, they are simple to express and very efficient on machines.
    </p>
    <p>
      Sometimes even linearity is desired because of some characteristics of the data. Sound waves are such a type of data because they satisfy the superposition property. That means that the result of the superposition of two waves is equivalent to the sum of the single waves. In such a case, it is natural to use linear operators because linearity assures the validity of the superposition principle for the processed waves too.
    </p>
</section>
<section>
  <h1 id="generalizations">Generalizations</h1>
  <p>
    In mathematics, a convolution is an operation defined between functions of a real variable. This continuous form is a generalization of the discrete convolution presented.
  </p>
  <p style="background-color:#d8deea; padding:10px;">
    <strong style="color:#434498; padding-bottom:10px">DEFINITION</strong></br>
    Let $f$, $g$ be two real function, the <strong>convolution</strong> of $f$ and $g$ is
    $$
    (f * g)(x) = \int_{-\infty}^{+\infty}f(t)g(x-t) dt  \quad \forall\: x \in \mathbb{R}.
    $$
  </p>
  <p>
    All the theorems we have proved hold for the continuous case too, replacing the summation symbol with the integral sign and signals with functions or tempered distributions.
    <small>
      Tempered distributions are a generalization of functions. The Dirac delta is characterized by the property $\int_{-\infty}^{+\infty} f(x) \delta(x) dx = f(0)$. No function can satisfy this property, but a tempered distribution can.
    </small>
  </p>
  <p>
    Data and signals can have many dimensions: a sound wave is monodimensional; a grayscale image has two dimensions; a color image has several channels that compose the third dimension.
    We can define the discrete convolution for signals of arbitrary dimension and prove all the results following the same steps. This generalization extends the main theorem to all kinds of data, in particular to images where convolutions have gained their fame. We report here the definition of convolution for the general case.
  </p>
  <p style="background-color:#d8deea; padding:10px;">
    <strong style="color:#434498; padding-bottom:10px">DEFINITION</strong></br>
    Let $f$, $g: \mathbb{Z}^{d} \longrightarrow \mathbb{R}$ be two $d$-dimensional signals,
    the <strong>convolution</strong> of $f$ and $g$ is
    $$
    (f * g)[n] = \sum_{k \in \mathbb{Z}^{d}}f[k]g[n-k]  \quad \forall\: n \in \mathbb{Z}^{d}.
    $$
  </p>
</section>
<section>
    <h1 id="cnn">Convolutional Neural Networks</h1>
    <p>
      The convolution operation has become a fundamental operation in the context of deep learning, in particular in the field of computer vision.
      A Convolutional Neural Network (CNN) is a sequence of 3D convolutions and pooling operations. Pooling operations are non-linear, and this allows the network to learn non-linear relationships between input and output. The most common ones are max-pooling and average-pooling.
    </p>
    <p>
      Images are a canonical example of a signal that necessitates shift-invariant operators, so it is interesting to know if CNNs are shift-invariant.
    </p>
    <p>
      Both convolutions and pooling operations perform the same computation on all equally-sized small portions of the input signal. Such calculations are independent of the absolute location of the values, and thus, shift-invariant, at one condition. The stride of the rolling window, the difference between two consecutive positions of the window, has to be 1. A convolution with stride greater than 1 is equivalent to a classic convolution followed by a sub-sampling. The sampling picks only a specific subset of the processed signal, and this selection depends on the absolute location of values. For example, if the stride is 2, values in even or odd positions are chosen.
    </p>
    <p>
      Currently, most successful CNNs relies on pooling operations with stride 2. Therefore they do not represent a shift-invariant operation!
      A researcher has noticed this flaw, and he has proposed alternative pooling operations as a replacement in the paper "<a href=https://arxiv.org/abs/1904.11486>Making Convolutional Networks Shift-Invariant Again</a>".
    </p>
</section>
<section>
    <h1 id="conclusions">Conclusions</h1>
    <p>
      The convolution operation in machine learning does not come out-of-the-blue. It emerges naturally from simple principles and assumptions on data: linearity and shift-invariance. Machine learning often seems to progress only through empirical experiments and chance, but many foundational components have a solid theory behind. We have clarified the reason behind the extensive utilization of convolutions in many applications.
    </p>
</section>
<footer style="text-align: left">
  <section>
    <h1>References</h1>
    <ol>
      <li id="ref-1">
        Mallat, Stéphane. <em>A wavelet tour of signal processing</em>. Elsevier, 1999.
      </li>
      <li id="ref-2">
        Zhang, Richard. <em>Making convolutional networks shift-invariant again</em>. arXiv preprint arXiv:1904.11486 (2019).
      </li>
    </ol>
  </section>
  <section>
    <h1>Updates and Corrections</h1>
    <p>If you see mistakes or want to suggest changes, please <a href="https://github.com/lucagrementieri/lucagrementieri.github.io">open an issue on GitHub</a>.</p>
  </section>
</footer>
